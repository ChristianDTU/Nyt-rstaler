{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this document, chatGPT has been used to optimize readability. \n",
    "# The convolutional neural network has been inspired by Mikkel N. Schmidt's example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "\n",
    "#%% Import libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Test_og_Train_df\\\\train_dataframe.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load sentences and labels\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTest_og_Train_df\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtrain_dataframe.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest_og_Train_df\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest_dataframe.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Test_og_Train_df\\\\train_dataframe.csv'"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "learning_rate = 0.001 \n",
    "weight_decay = 1e-4\n",
    "batch_size = 128\n",
    "\n",
    "# Load sentences and labels\n",
    "train = pd.read_csv(r'Test_og_Train_df\\train_dataframe.csv')\n",
    "test = pd.read_csv(r'Test_og_Train_df\\test_dataframe.csv')\n",
    "\n",
    "train_labels = train[\"Labels\"].tolist()\n",
    "test_labels = test[\"Labels\"].tolist()\n",
    "\n",
    "train_tf_idf = pd.read_csv(r'DTTFIDFM_data\\DTTFIDFM_train.csv')\n",
    "test_tf_idf = pd.read_csv(r'DTTFIDFM_data\\DTTFIDFM_test.csv')\n",
    "\n",
    "train_tf_idf = train_tf_idf.drop('Unnamed: 0', axis=1)\n",
    "test_tf_idf = test_tf_idf.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "train_tf_idf_numpy = train_tf_idf.to_numpy()\n",
    "test_tf_idf_numpy = test_tf_idf.to_numpy()\n",
    "\n",
    "# Sentence LSA embedding\n",
    "\n",
    "def sentence_LSA_embedding(TF_IDF_matrix_numpy, V_k):\n",
    "    liste_LSA_vektorer = []\n",
    "    for i in range(TF_IDF_matrix_numpy.shape[0]):\n",
    "        lsa_vector = np.dot(TF_IDF_matrix_numpy[i], V_k)\n",
    "        liste_LSA_vektorer.append(lsa_vector)\n",
    "    return np.array(liste_LSA_vektorer)\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001 \n",
    "weight_decay = 1e-4 \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural netvÃ¦rk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetSS:\n",
    "    def __init__(self, input_dim, con_layers, num_of_lin_lay):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.input_dim = input_dim   # Initial input dimension\n",
    "        self.con_layers = con_layers  # Number of convolutional layers\n",
    "        self.num_of_lin_lay = num_of_lin_lay  # Number of linear layers\n",
    "    \n",
    "    def conv_block(self):\n",
    "        \"\"\"Creates a convolutional block based on `con_layers`.\"\"\"\n",
    "        layers = []\n",
    "        current_dim = self.input_dim\n",
    "        \n",
    "        if self.con_layers >= 1:\n",
    "            layers.append(torch.nn.Conv1d(1, 16, kernel_size=3))  # Conv 1\n",
    "            current_dim = (current_dim - 3 + 1) // 2  # After pooling\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(p=0.05))\n",
    "            layers.append(torch.nn.MaxPool1d(kernel_size=2))\n",
    "        \n",
    "        if self.con_layers >= 2:\n",
    "            layers.append(torch.nn.Conv1d(16, 16, kernel_size=3))  # Conv 2\n",
    "            current_dim = (current_dim - 3 + 1) // 2  # After pooling\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(p=0.05))\n",
    "            layers.append(torch.nn.MaxPool1d(kernel_size=2))\n",
    "        \n",
    "        layers.append(torch.nn.Flatten())\n",
    "        self.output_dim = 16 * current_dim  # Update for final linear input dimension\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def linear_layers(self):\n",
    "        \"\"\"Creates the linear layers dynamically based on `num_of_lin_lay`.\"\"\"\n",
    "        layers = []\n",
    "        current_dim = self.output_dim\n",
    "\n",
    "        for i in range(self.num_of_lin_lay - 1):\n",
    "            next_dim = current_dim // 2\n",
    "            layers.append(torch.nn.Linear(current_dim, next_dim))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            current_dim = next_dim\n",
    "        \n",
    "        # Final output layer\n",
    "        layers.append(torch.nn.Linear(current_dim, 2))\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Builds the model based on `con_layers` and `num_of_lin_lay`.\"\"\"\n",
    "        model = torch.nn.Sequential(\n",
    "            self.conv_block(),\n",
    "            self.linear_layers()\n",
    "        )\n",
    "        return model.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load:\n",
    "    def __init__(self,the_matrix,net):\n",
    "        self.the_matrix = the_matrix\n",
    "\n",
    "        train_lsa = sentence_LSA_embedding(train_tf_idf_numpy, the_matrix)\n",
    "        test_lsa = sentence_LSA_embedding(test_tf_idf_numpy, the_matrix)\n",
    "\n",
    "        #Tensors\n",
    "\n",
    "        # Convert inputs\n",
    "        train_input_tensor = torch.tensor(train_lsa, dtype=torch.float32)  \n",
    "        test_input_tensor = torch.tensor(test_lsa, dtype=torch.float32)   \n",
    "\n",
    "        # Convert labels\n",
    "        train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)  # Shape: (num_train_sentences,)\n",
    "        test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)    # Shape: (num_test_sentences,)\n",
    "\n",
    "        # Reshape the tensors for CNN input (adding the channel dimension)\n",
    "        train_input_tensor = train_input_tensor.unsqueeze(1)  # Shape: (num_train_sentences, 1, 200)\n",
    "        test_input_tensor = test_input_tensor.unsqueeze(1)    # Shape: (num_test_sentences, 1, 200)\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = TensorDataset(train_input_tensor, train_labels_tensor)\n",
    "        test_dataset = TensorDataset(test_input_tensor, test_labels_tensor)\n",
    "\n",
    "        # Create dataloaders\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Shuffle training data\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)   # No shuffle for testing\n",
    "\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Train\n",
    "class Train:\n",
    "    def __init__(self, matrix, net):\n",
    "        self.matrix = matrix\n",
    "\n",
    "        train_loss = {}\n",
    "        test_loss = {}\n",
    "        train_accuracy = {}\n",
    "        test_accuracy = {}\n",
    "        step = 0\n",
    "        Loader = Load(self.matrix, net)\n",
    "        best_test_acc = 0\n",
    "\n",
    "        #scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for inputs, labels in Loader.train_loader:\n",
    "                net.train()\n",
    "\n",
    "                # Put data on GPU \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                Loader.optimizer.zero_grad()\n",
    "\n",
    "                # Compute loss and take gradient step\n",
    "                outputs = net(inputs)\n",
    "                loss = Loader.loss_function(outputs, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                Loader.optimizer.step()\n",
    "\n",
    "                predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Print accuracy for epoch            \n",
    "            epoch_loss = running_loss / len(Loader.train_loader)\n",
    "\n",
    "            train_loss[epoch] = epoch_loss\n",
    "\n",
    "            epoch_accuracy = 100 * correct / total\n",
    "\n",
    "            train_accuracy[epoch] = epoch_accuracy\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Training loss: {epoch_loss:.4f}, Training accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "            # Evaluate the model\n",
    "            net.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            running_loss_test = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in Loader.test_loader:\n",
    "                    # Put data on GPU \n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = net(inputs)\n",
    "                    \n",
    "                    predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                    loss = Loader.loss_function(outputs, labels)\n",
    "                    \n",
    "                    running_loss_test += loss.item()\n",
    "\n",
    "                    # Count correct predictions\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                epoch_loss = running_loss_test / len(Loader.test_loader)\n",
    "                test_loss[epoch] = epoch_loss\n",
    "\n",
    "            test_accuracy_score = 100 * correct / total\n",
    "            \n",
    "            test_accuracy[epoch] = test_accuracy_score\n",
    "\n",
    "            print(f\"Test loss: {epoch_loss:.4f}, Test accuracy: {test_accuracy_score:.2f}%\")\n",
    "            print()\n",
    "\n",
    "            #scheduler.step()\n",
    "            \n",
    "            if test_accuracy[epoch] > best_test_acc:\n",
    "                best_test_acc = test_accuracy[epoch]\n",
    "                print(f'best test accuracy so far: {best_test_acc}')\n",
    "\n",
    "        print(f'best test accuracy: {best_test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with V300 matrix, 1 conv layers and 1 linear layers.\n",
      "Epoch 1/100\n",
      "Training loss: 0.6172, Training accuracy: 68.24%\n",
      "Test loss: 0.6074, Test accuracy: 70.63%\n",
      "\n",
      "best test accuracy so far: 70.62989191203876\n",
      "Epoch 2/100\n",
      "Training loss: 0.5922, Training accuracy: 68.94%\n",
      "Test loss: 0.5849, Test accuracy: 70.63%\n",
      "\n",
      "Epoch 3/100\n",
      "Training loss: 0.5757, Training accuracy: 69.26%\n",
      "Test loss: 0.5954, Test accuracy: 70.33%\n",
      "\n",
      "Epoch 4/100\n",
      "Training loss: 0.5728, Training accuracy: 69.31%\n",
      "Test loss: 0.5968, Test accuracy: 71.90%\n",
      "\n",
      "best test accuracy so far: 71.89713007827059\n",
      "Epoch 5/100\n",
      "Training loss: 0.5615, Training accuracy: 69.77%\n",
      "Test loss: 0.5840, Test accuracy: 71.64%\n",
      "\n",
      "Epoch 6/100\n",
      "Training loss: 0.5568, Training accuracy: 70.48%\n",
      "Test loss: 0.6088, Test accuracy: 69.36%\n",
      "\n",
      "Epoch 7/100\n",
      "Training loss: 0.5492, Training accuracy: 71.14%\n",
      "Test loss: 0.5524, Test accuracy: 71.52%\n",
      "\n",
      "Epoch 8/100\n",
      "Training loss: 0.5367, Training accuracy: 71.96%\n",
      "Test loss: 0.5477, Test accuracy: 72.94%\n",
      "\n",
      "best test accuracy so far: 72.94073797987328\n",
      "Epoch 9/100\n",
      "Training loss: 0.5247, Training accuracy: 73.56%\n",
      "Test loss: 0.5438, Test accuracy: 75.07%\n",
      "\n",
      "best test accuracy so far: 75.06522549385016\n",
      "Epoch 10/100\n",
      "Training loss: 0.5129, Training accuracy: 74.40%\n",
      "Test loss: 0.5280, Test accuracy: 74.43%\n",
      "\n",
      "Epoch 11/100\n",
      "Training loss: 0.5020, Training accuracy: 75.34%\n",
      "Test loss: 0.5298, Test accuracy: 76.11%\n",
      "\n",
      "best test accuracy so far: 76.10883339545285\n",
      "Epoch 12/100\n",
      "Training loss: 0.4969, Training accuracy: 75.58%\n",
      "Test loss: 0.5246, Test accuracy: 76.59%\n",
      "\n",
      "best test accuracy so far: 76.59336563548267\n",
      "Epoch 13/100\n",
      "Training loss: 0.4908, Training accuracy: 76.32%\n",
      "Test loss: 0.5168, Test accuracy: 76.37%\n",
      "\n",
      "Epoch 14/100\n",
      "Training loss: 0.4868, Training accuracy: 76.68%\n",
      "Test loss: 0.5102, Test accuracy: 75.81%\n",
      "\n",
      "Epoch 15/100\n",
      "Training loss: 0.4830, Training accuracy: 76.53%\n",
      "Test loss: 0.5281, Test accuracy: 75.10%\n",
      "\n",
      "Epoch 16/100\n",
      "Training loss: 0.4823, Training accuracy: 76.28%\n",
      "Test loss: 0.5234, Test accuracy: 75.48%\n",
      "\n",
      "Epoch 17/100\n",
      "Training loss: 0.4764, Training accuracy: 77.51%\n",
      "Test loss: 0.5388, Test accuracy: 73.09%\n",
      "\n",
      "Epoch 18/100\n",
      "Training loss: 0.4734, Training accuracy: 77.35%\n",
      "Test loss: 0.5023, Test accuracy: 76.63%\n",
      "\n",
      "best test accuracy so far: 76.63063734625419\n",
      "Epoch 19/100\n",
      "Training loss: 0.4717, Training accuracy: 77.36%\n",
      "Test loss: 0.5131, Test accuracy: 76.18%\n",
      "\n",
      "Epoch 20/100\n",
      "Training loss: 0.4686, Training accuracy: 77.72%\n",
      "Test loss: 0.4995, Test accuracy: 76.74%\n",
      "\n",
      "best test accuracy so far: 76.74245247856877\n",
      "Epoch 21/100\n",
      "Training loss: 0.4735, Training accuracy: 76.67%\n",
      "Test loss: 0.5095, Test accuracy: 74.80%\n",
      "\n",
      "Epoch 22/100\n",
      "Training loss: 0.4637, Training accuracy: 78.12%\n",
      "Test loss: 0.4991, Test accuracy: 76.63%\n",
      "\n",
      "Epoch 23/100\n",
      "Training loss: 0.4640, Training accuracy: 77.98%\n",
      "Test loss: 0.5119, Test accuracy: 75.48%\n",
      "\n",
      "Epoch 24/100\n",
      "Training loss: 0.4678, Training accuracy: 76.68%\n",
      "Test loss: 0.5069, Test accuracy: 75.25%\n",
      "\n",
      "Epoch 25/100\n",
      "Training loss: 0.4608, Training accuracy: 77.96%\n",
      "Test loss: 0.5094, Test accuracy: 75.70%\n",
      "\n",
      "Epoch 26/100\n",
      "Training loss: 0.4629, Training accuracy: 77.51%\n",
      "Test loss: 0.5057, Test accuracy: 76.18%\n",
      "\n",
      "Epoch 27/100\n",
      "Training loss: 0.4578, Training accuracy: 78.22%\n",
      "Test loss: 0.4931, Test accuracy: 77.15%\n",
      "\n",
      "best test accuracy so far: 77.15244129705553\n",
      "Epoch 28/100\n",
      "Training loss: 0.4568, Training accuracy: 78.21%\n",
      "Test loss: 0.5304, Test accuracy: 72.75%\n",
      "\n",
      "Epoch 29/100\n",
      "Training loss: 0.4530, Training accuracy: 78.49%\n",
      "Test loss: 0.4905, Test accuracy: 77.04%\n",
      "\n",
      "Epoch 30/100\n",
      "Training loss: 0.4570, Training accuracy: 78.23%\n",
      "Test loss: 0.5048, Test accuracy: 76.11%\n",
      "\n",
      "Epoch 31/100\n",
      "Training loss: 0.4525, Training accuracy: 78.60%\n",
      "Test loss: 0.5057, Test accuracy: 75.92%\n",
      "\n",
      "Epoch 32/100\n",
      "Training loss: 0.4520, Training accuracy: 78.53%\n",
      "Test loss: 0.4976, Test accuracy: 76.85%\n",
      "\n",
      "Epoch 33/100\n",
      "Training loss: 0.4506, Training accuracy: 78.85%\n",
      "Test loss: 0.5127, Test accuracy: 75.03%\n",
      "\n",
      "Epoch 34/100\n",
      "Training loss: 0.4526, Training accuracy: 78.78%\n",
      "Test loss: 0.4947, Test accuracy: 77.19%\n",
      "\n",
      "best test accuracy so far: 77.18971300782707\n",
      "Epoch 35/100\n",
      "Training loss: 0.4484, Training accuracy: 78.95%\n",
      "Test loss: 0.4923, Test accuracy: 77.26%\n",
      "\n",
      "best test accuracy so far: 77.2642564293701\n",
      "Epoch 36/100\n",
      "Training loss: 0.4454, Training accuracy: 79.21%\n",
      "Test loss: 0.5095, Test accuracy: 75.07%\n",
      "\n",
      "Epoch 37/100\n",
      "Training loss: 0.4469, Training accuracy: 78.81%\n",
      "Test loss: 0.4862, Test accuracy: 77.30%\n",
      "\n",
      "best test accuracy so far: 77.30152814014163\n",
      "Epoch 38/100\n",
      "Training loss: 0.4448, Training accuracy: 78.98%\n",
      "Test loss: 0.4892, Test accuracy: 77.38%\n",
      "\n",
      "best test accuracy so far: 77.37607156168468\n",
      "Epoch 39/100\n",
      "Training loss: 0.4431, Training accuracy: 78.99%\n",
      "Test loss: 0.5019, Test accuracy: 76.07%\n",
      "\n",
      "Epoch 40/100\n",
      "Training loss: 0.4470, Training accuracy: 78.85%\n",
      "Test loss: 0.4901, Test accuracy: 77.26%\n",
      "\n",
      "Epoch 41/100\n",
      "Training loss: 0.4430, Training accuracy: 79.22%\n",
      "Test loss: 0.4875, Test accuracy: 77.34%\n",
      "\n",
      "Epoch 42/100\n",
      "Training loss: 0.4454, Training accuracy: 79.16%\n",
      "Test loss: 0.4846, Test accuracy: 77.41%\n",
      "\n",
      "best test accuracy so far: 77.4133432724562\n",
      "Epoch 43/100\n",
      "Training loss: 0.4427, Training accuracy: 79.26%\n",
      "Test loss: 0.4884, Test accuracy: 77.30%\n",
      "\n",
      "Epoch 44/100\n",
      "Training loss: 0.4405, Training accuracy: 79.45%\n",
      "Test loss: 0.5030, Test accuracy: 75.66%\n",
      "\n",
      "Epoch 45/100\n",
      "Training loss: 0.4386, Training accuracy: 79.41%\n",
      "Test loss: 0.5052, Test accuracy: 75.62%\n",
      "\n",
      "Epoch 46/100\n",
      "Training loss: 0.4361, Training accuracy: 79.80%\n",
      "Test loss: 0.4877, Test accuracy: 77.08%\n",
      "\n",
      "Epoch 47/100\n",
      "Training loss: 0.4387, Training accuracy: 78.90%\n",
      "Test loss: 0.4858, Test accuracy: 77.49%\n",
      "\n",
      "best test accuracy so far: 77.48788669399926\n",
      "Epoch 48/100\n",
      "Training loss: 0.4383, Training accuracy: 79.50%\n",
      "Test loss: 0.4982, Test accuracy: 76.18%\n",
      "\n",
      "Epoch 49/100\n",
      "Training loss: 0.4363, Training accuracy: 79.63%\n",
      "Test loss: 0.5276, Test accuracy: 72.57%\n",
      "\n",
      "Epoch 50/100\n",
      "Training loss: 0.4367, Training accuracy: 79.70%\n",
      "Test loss: 0.5384, Test accuracy: 70.93%\n",
      "\n",
      "Epoch 51/100\n",
      "Training loss: 0.4396, Training accuracy: 78.94%\n",
      "Test loss: 0.4818, Test accuracy: 77.49%\n",
      "\n",
      "Epoch 52/100\n",
      "Training loss: 0.4380, Training accuracy: 79.47%\n",
      "Test loss: 0.4829, Test accuracy: 77.82%\n",
      "\n",
      "best test accuracy so far: 77.82333209094297\n",
      "Epoch 53/100\n",
      "Training loss: 0.4356, Training accuracy: 79.06%\n",
      "Test loss: 0.4887, Test accuracy: 76.78%\n",
      "\n",
      "Epoch 54/100\n",
      "Training loss: 0.4341, Training accuracy: 79.45%\n",
      "Test loss: 0.4886, Test accuracy: 77.26%\n",
      "\n",
      "Epoch 55/100\n",
      "Training loss: 0.4347, Training accuracy: 79.52%\n",
      "Test loss: 0.4870, Test accuracy: 76.89%\n",
      "\n",
      "Epoch 56/100\n",
      "Training loss: 0.4354, Training accuracy: 79.39%\n",
      "Test loss: 0.4831, Test accuracy: 77.49%\n",
      "\n",
      "Epoch 57/100\n",
      "Training loss: 0.4312, Training accuracy: 79.93%\n",
      "Test loss: 0.4841, Test accuracy: 77.34%\n",
      "\n",
      "Epoch 58/100\n",
      "Training loss: 0.4329, Training accuracy: 79.55%\n",
      "Test loss: 0.4803, Test accuracy: 77.94%\n",
      "\n",
      "best test accuracy so far: 77.93514722325754\n",
      "Epoch 59/100\n",
      "Training loss: 0.4309, Training accuracy: 79.76%\n",
      "Test loss: 0.5004, Test accuracy: 75.55%\n",
      "\n",
      "Epoch 60/100\n",
      "Training loss: 0.4323, Training accuracy: 79.42%\n",
      "Test loss: 0.4863, Test accuracy: 76.97%\n",
      "\n",
      "Epoch 61/100\n",
      "Training loss: 0.4291, Training accuracy: 80.31%\n",
      "Test loss: 0.4885, Test accuracy: 76.89%\n",
      "\n",
      "Epoch 62/100\n",
      "Training loss: 0.4303, Training accuracy: 79.93%\n",
      "Test loss: 0.4795, Test accuracy: 77.75%\n",
      "\n",
      "Epoch 63/100\n",
      "Training loss: 0.4339, Training accuracy: 79.55%\n",
      "Test loss: 0.4850, Test accuracy: 77.19%\n",
      "\n",
      "Epoch 64/100\n",
      "Training loss: 0.4269, Training accuracy: 80.31%\n",
      "Test loss: 0.4866, Test accuracy: 76.89%\n",
      "\n",
      "Epoch 65/100\n",
      "Training loss: 0.4325, Training accuracy: 79.85%\n",
      "Test loss: 0.4826, Test accuracy: 77.56%\n",
      "\n",
      "Epoch 66/100\n",
      "Training loss: 0.4262, Training accuracy: 80.06%\n",
      "Test loss: 0.4998, Test accuracy: 75.55%\n",
      "\n",
      "Epoch 67/100\n",
      "Training loss: 0.4297, Training accuracy: 80.00%\n",
      "Test loss: 0.5036, Test accuracy: 75.55%\n",
      "\n",
      "Epoch 68/100\n",
      "Training loss: 0.4282, Training accuracy: 79.76%\n",
      "Test loss: 0.4845, Test accuracy: 77.34%\n",
      "\n",
      "Epoch 69/100\n",
      "Training loss: 0.4292, Training accuracy: 79.82%\n",
      "Test loss: 0.4999, Test accuracy: 75.59%\n",
      "\n",
      "Epoch 70/100\n",
      "Training loss: 0.4254, Training accuracy: 80.12%\n",
      "Test loss: 0.4923, Test accuracy: 76.33%\n",
      "\n",
      "Epoch 71/100\n",
      "Training loss: 0.4265, Training accuracy: 79.83%\n",
      "Test loss: 0.4791, Test accuracy: 77.86%\n",
      "\n",
      "Epoch 72/100\n",
      "Training loss: 0.4251, Training accuracy: 80.09%\n",
      "Test loss: 0.4783, Test accuracy: 77.67%\n",
      "\n",
      "Epoch 73/100\n",
      "Training loss: 0.4326, Training accuracy: 79.57%\n",
      "Test loss: 0.4850, Test accuracy: 77.08%\n",
      "\n",
      "Epoch 74/100\n",
      "Training loss: 0.4210, Training accuracy: 80.59%\n",
      "Test loss: 0.4898, Test accuracy: 76.48%\n",
      "\n",
      "Epoch 75/100\n",
      "Training loss: 0.4242, Training accuracy: 80.18%\n",
      "Test loss: 0.4895, Test accuracy: 76.26%\n",
      "\n",
      "Epoch 76/100\n",
      "Training loss: 0.4276, Training accuracy: 79.82%\n",
      "Test loss: 0.4873, Test accuracy: 76.74%\n",
      "\n",
      "Epoch 77/100\n",
      "Training loss: 0.4233, Training accuracy: 80.04%\n",
      "Test loss: 0.4939, Test accuracy: 76.44%\n",
      "\n",
      "Epoch 78/100\n",
      "Training loss: 0.4222, Training accuracy: 80.21%\n",
      "Test loss: 0.4820, Test accuracy: 77.34%\n",
      "\n",
      "Epoch 79/100\n",
      "Training loss: 0.4251, Training accuracy: 80.21%\n",
      "Test loss: 0.4948, Test accuracy: 76.56%\n",
      "\n",
      "Epoch 80/100\n",
      "Training loss: 0.4287, Training accuracy: 79.67%\n",
      "Test loss: 0.4798, Test accuracy: 77.82%\n",
      "\n",
      "Epoch 81/100\n",
      "Training loss: 0.4242, Training accuracy: 80.68%\n",
      "Test loss: 0.4790, Test accuracy: 77.82%\n",
      "\n",
      "Epoch 82/100\n",
      "Training loss: 0.4235, Training accuracy: 80.54%\n",
      "Test loss: 0.4912, Test accuracy: 76.52%\n",
      "\n",
      "Epoch 83/100\n",
      "Training loss: 0.4216, Training accuracy: 80.19%\n",
      "Test loss: 0.4771, Test accuracy: 77.97%\n",
      "\n",
      "best test accuracy so far: 77.97241893402906\n",
      "Epoch 84/100\n",
      "Training loss: 0.4215, Training accuracy: 80.55%\n",
      "Test loss: 0.4844, Test accuracy: 76.97%\n",
      "\n",
      "Epoch 85/100\n",
      "Training loss: 0.4209, Training accuracy: 80.75%\n",
      "Test loss: 0.4893, Test accuracy: 76.74%\n",
      "\n",
      "Epoch 86/100\n",
      "Training loss: 0.4196, Training accuracy: 80.64%\n",
      "Test loss: 0.4767, Test accuracy: 78.05%\n",
      "\n",
      "best test accuracy so far: 78.04696235557212\n",
      "Epoch 87/100\n",
      "Training loss: 0.4261, Training accuracy: 79.76%\n",
      "Test loss: 0.4825, Test accuracy: 77.45%\n",
      "\n",
      "Epoch 88/100\n",
      "Training loss: 0.4217, Training accuracy: 80.70%\n",
      "Test loss: 0.4889, Test accuracy: 76.52%\n",
      "\n",
      "Epoch 89/100\n",
      "Training loss: 0.4191, Training accuracy: 80.59%\n",
      "Test loss: 0.4795, Test accuracy: 77.75%\n",
      "\n",
      "Epoch 90/100\n",
      "Training loss: 0.4157, Training accuracy: 80.50%\n",
      "Test loss: 0.4927, Test accuracy: 76.59%\n",
      "\n",
      "Epoch 91/100\n",
      "Training loss: 0.4191, Training accuracy: 80.16%\n",
      "Test loss: 0.4845, Test accuracy: 76.82%\n",
      "\n",
      "Epoch 92/100\n",
      "Training loss: 0.4168, Training accuracy: 80.91%\n",
      "Test loss: 0.4805, Test accuracy: 77.60%\n",
      "\n",
      "Epoch 93/100\n",
      "Training loss: 0.4201, Training accuracy: 80.12%\n",
      "Test loss: 0.4787, Test accuracy: 77.71%\n",
      "\n",
      "Epoch 94/100\n",
      "Training loss: 0.4161, Training accuracy: 80.53%\n",
      "Test loss: 0.4803, Test accuracy: 77.60%\n",
      "\n",
      "Epoch 95/100\n",
      "Training loss: 0.4176, Training accuracy: 80.62%\n",
      "Test loss: 0.4790, Test accuracy: 77.86%\n",
      "\n",
      "Epoch 96/100\n",
      "Training loss: 0.4175, Training accuracy: 80.52%\n",
      "Test loss: 0.4802, Test accuracy: 77.49%\n",
      "\n",
      "Epoch 97/100\n",
      "Training loss: 0.4189, Training accuracy: 80.13%\n",
      "Test loss: 0.4776, Test accuracy: 78.05%\n",
      "\n",
      "Epoch 98/100\n",
      "Training loss: 0.4135, Training accuracy: 80.84%\n",
      "Test loss: 0.4752, Test accuracy: 78.23%\n",
      "\n",
      "best test accuracy so far: 78.23332090942974\n",
      "Epoch 99/100\n",
      "Training loss: 0.4173, Training accuracy: 80.54%\n",
      "Test loss: 0.4790, Test accuracy: 77.53%\n",
      "\n",
      "Epoch 100/100\n",
      "Training loss: 0.4152, Training accuracy: 80.64%\n",
      "Test loss: 0.4803, Test accuracy: 77.26%\n",
      "\n",
      "best test accuracy: 78.23332090942974\n",
      "Testing with V300 matrix, 2 conv layers and 1 linear layers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m net \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbuild_model()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Load the current matrix and train\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Integrate this with your training framework        \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[44], line 11\u001b[0m, in \u001b[0;36mTrain.__init__\u001b[1;34m(self, matrix, net)\u001b[0m\n\u001b[0;32m      9\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     10\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m Loader \u001b[38;5;241m=\u001b[39m \u001b[43mLoad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m best_test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 5\u001b[0m, in \u001b[0;36mLoad.__init__\u001b[1;34m(self, the_matrix)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,the_matrix):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthe_matrix \u001b[38;5;241m=\u001b[39m the_matrix\n\u001b[1;32m----> 5\u001b[0m     train_lsa \u001b[38;5;241m=\u001b[39m \u001b[43msentence_LSA_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tf_idf_numpy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthe_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     test_lsa \u001b[38;5;241m=\u001b[39m sentence_LSA_embedding(test_tf_idf_numpy, the_matrix)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#Tensors\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Convert inputs\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[36], line 27\u001b[0m, in \u001b[0;36msentence_LSA_embedding\u001b[1;34m(TF_IDF_matrix_numpy, V_k)\u001b[0m\n\u001b[0;32m     25\u001b[0m liste_LSA_vektorer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TF_IDF_matrix_numpy\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m---> 27\u001b[0m     lsa_vector \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTF_IDF_matrix_numpy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     liste_LSA_vektorer\u001b[38;5;241m.\u001b[39mappend(lsa_vector)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(liste_LSA_vektorer)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SVD_matricer = ['V200, V300', 'V500', 'A_k200', 'A_k300', 'A_k500']\n",
    "lin_layers = [1,2,3,4,5]\n",
    "dimensioner = [200, 300, 500, 200, 300, 500]\n",
    "conv_layers = [1,2]\n",
    "\n",
    "for x in range(len(SVD_matricer)):\n",
    "    current_matrix = genfromtxt(f'{SVD_matricer[x]}.csv', delimiter=',')  # Adjust path as needed\n",
    "    for i in lin_layers:\n",
    "        for j in conv_layers:\n",
    "            try:\n",
    "                print(f\"Testing with {SVD_matricer[x]} matrix, {j} conv layers and {i} linear layers.\")\n",
    "                # Initialize and build the model\n",
    "                model = NetSS(input_dim=dimensioner[x], con_layers=j, num_of_lin_lay=i)\n",
    "                net = model.build_model()\n",
    "\n",
    "                # Load the current matrix and train\n",
    "                Train(current_matrix, net)  # Integrate this with your training framework\n",
    "            except Exception as e:\n",
    "                print(f\"Error encountered with {SVD_matricer[x]} matrix, {j} conv layers and {i} linear layers: {e}\")       \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
