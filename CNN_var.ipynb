{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numpy import genfromtxt\n",
    "\n",
    "#%% Import libraries\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as plt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "V300.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m train_tf_idf_numpy \u001b[38;5;241m=\u001b[39m train_tf_idf\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m     22\u001b[0m test_tf_idf_numpy \u001b[38;5;241m=\u001b[39m test_tf_idf\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m---> 24\u001b[0m current_matrix \u001b[38;5;241m=\u001b[39m genfromtxt(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSVD_matricer[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Load SVD Matrix\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Sentence LSA embedding\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence_LSA_embedding\u001b[39m(TF_IDF_matrix_numpy, V_k):\n",
      "File \u001b[1;32mc:\\Users\\chris\\Miniconda3\\envs\\IntelligentSystems\\Lib\\site-packages\\numpy\\lib\\npyio.py:1980\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[0;32m   1978\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os_fspath(fname)\n\u001b[0;32m   1979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1980\u001b[0m     fid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39m_datasource\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[0;32m   1981\u001b[0m     fid_ctx \u001b[38;5;241m=\u001b[39m contextlib\u001b[38;5;241m.\u001b[39mclosing(fid)\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chris\\Miniconda3\\envs\\IntelligentSystems\\Lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mopen(path, mode, encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n",
      "File \u001b[1;32mc:\\Users\\chris\\Miniconda3\\envs\\IntelligentSystems\\Lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: V300.csv not found."
     ]
    }
   ],
   "source": [
    "SVD_matricer = ['V300', 'V500', 'A_k200', 'A_k300', 'A_k500']\n",
    "dimensioner = [300, 500, 200, 300, 500]\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001 \n",
    "weight_decay = 1e-4\n",
    "batch_size = 128\n",
    "\n",
    "# Load sentences and labels\n",
    "train = pd.read_csv(r'C:\\Users\\chris\\Documents\\GitHub\\Nyt-rstaler\\Test_og_Train_df\\train_dataframe.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\chris\\Documents\\GitHub\\Nyt-rstaler\\Test_og_Train_df\\test_dataframe.csv')\n",
    "\n",
    "train_labels = train[\"Labels\"].tolist()\n",
    "test_labels = test[\"Labels\"].tolist()\n",
    "\n",
    "train_tf_idf = pd.read_csv(r'C:\\Users\\chris\\Documents\\GitHub\\Nyt-rstaler\\DTTFIDFM_data\\DTTFIDFM_train.csv')\n",
    "test_tf_idf = pd.read_csv(r'C:\\Users\\chris\\Documents\\GitHub\\Nyt-rstaler\\DTTFIDFM_data\\DTTFIDFM_test.csv')\n",
    "\n",
    "train_tf_idf = train_tf_idf.drop('Unnamed: 0', axis=1)\n",
    "test_tf_idf = test_tf_idf.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "train_tf_idf_numpy = train_tf_idf.to_numpy()\n",
    "test_tf_idf_numpy = test_tf_idf.to_numpy()\n",
    "\n",
    "current_matrix = genfromtxt(f'{SVD_matricer[0]}.csv', delimiter=',') # Load SVD Matrix\n",
    "\n",
    "# Sentence LSA embedding\n",
    "\n",
    "def sentence_LSA_embedding(TF_IDF_matrix_numpy, V_k):\n",
    "    liste_LSA_vektorer = []\n",
    "    for i in range(TF_IDF_matrix_numpy.shape[0]):\n",
    "        lsa_vector = np.dot(TF_IDF_matrix_numpy[i], V_k)\n",
    "        liste_LSA_vektorer.append(lsa_vector)\n",
    "    return np.array(liste_LSA_vektorer)\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001 \n",
    "weight_decay = 1e-4 \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural netvÃ¦rk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetSS:\n",
    "    def __init__(self, input_dim, con_layers, num_of_lin_lay):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.input_dim = input_dim   # Initial input dimension\n",
    "        self.con_layers = con_layers  # Number of convolutional layers\n",
    "        self.num_of_lin_lay = num_of_lin_lay  # Number of linear layers\n",
    "    \n",
    "    def conv_block(self):\n",
    "        \"\"\"Creates a convolutional block based on `con_layers`.\"\"\"\n",
    "        layers = []\n",
    "        current_dim = self.input_dim\n",
    "        \n",
    "        if self.con_layers >= 1:\n",
    "            layers.append(torch.nn.Conv1d(1, 16, kernel_size=3))  # Conv 1\n",
    "            current_dim = (current_dim - 3 + 1) // 2  # After pooling\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(p=0.05))\n",
    "            layers.append(torch.nn.MaxPool1d(kernel_size=2))\n",
    "        \n",
    "        if self.con_layers >= 2:\n",
    "            layers.append(torch.nn.Conv1d(16, 16, kernel_size=3))  # Conv 2\n",
    "            current_dim = (current_dim - 3 + 1) // 2  # After pooling\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(p=0.05))\n",
    "            layers.append(torch.nn.MaxPool1d(kernel_size=2))\n",
    "        \n",
    "        layers.append(torch.nn.Flatten())\n",
    "        self.output_dim = 16 * current_dim  # Update for final linear input dimension\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def linear_layers(self):\n",
    "        \"\"\"Creates the linear layers dynamically based on `num_of_lin_lay`.\"\"\"\n",
    "        layers = []\n",
    "        current_dim = self.output_dim\n",
    "\n",
    "        for i in range(self.num_of_lin_lay - 1):\n",
    "            next_dim = current_dim // 2\n",
    "            layers.append(torch.nn.Linear(current_dim, next_dim))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            current_dim = next_dim\n",
    "        \n",
    "        # Final output layer\n",
    "        layers.append(torch.nn.Linear(current_dim, 2))\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Builds the model based on `con_layers` and `num_of_lin_lay`.\"\"\"\n",
    "        model = torch.nn.Sequential(\n",
    "            self.conv_block(),\n",
    "            self.linear_layers()\n",
    "        )\n",
    "        return model.to(self.device)\n",
    "\n",
    "model = NetSS(dimensioner[0], 1, 1)\n",
    "net = model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load:\n",
    "    def __init__(self,the_matrix):\n",
    "        self.the_matrix = the_matrix\n",
    "\n",
    "        train_lsa = sentence_LSA_embedding(train_tf_idf_numpy, the_matrix)\n",
    "        test_lsa = sentence_LSA_embedding(test_tf_idf_numpy, the_matrix)\n",
    "\n",
    "        #Tensors\n",
    "\n",
    "        # Convert inputs\n",
    "        train_input_tensor = torch.tensor(train_lsa, dtype=torch.float32)  \n",
    "        test_input_tensor = torch.tensor(test_lsa, dtype=torch.float32)   \n",
    "\n",
    "        # Convert labels\n",
    "        train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)  # Shape: (num_train_sentences,)\n",
    "        test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)    # Shape: (num_test_sentences,)\n",
    "\n",
    "        # Reshape the tensors for CNN input (adding the channel dimension)\n",
    "        train_input_tensor = train_input_tensor.unsqueeze(1)  # Shape: (num_train_sentences, 1, 200)\n",
    "        test_input_tensor = test_input_tensor.unsqueeze(1)    # Shape: (num_test_sentences, 1, 200)\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = TensorDataset(train_input_tensor, train_labels_tensor)\n",
    "        test_dataset = TensorDataset(test_input_tensor, test_labels_tensor)\n",
    "\n",
    "        # Create dataloaders\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Shuffle training data\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)   # No shuffle for testing\n",
    "\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Train\n",
    "class Train:\n",
    "    def __init__(self, matrix):\n",
    "        self.matrix = matrix\n",
    "        pass\n",
    "\n",
    "        train_loss = {}\n",
    "        test_loss = {}\n",
    "        train_accuracy = {}\n",
    "        test_accuracy = {}\n",
    "        step = 0\n",
    "        Loader = Load(self.matrix)\n",
    "\n",
    "        #scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for inputs, labels in Loader.train_loader:\n",
    "                net.train()\n",
    "\n",
    "                # Put data on GPU \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                Loader.optimizer.zero_grad()\n",
    "\n",
    "                # Compute loss and take gradient step\n",
    "                outputs = net(inputs)\n",
    "                loss = Loader.loss_function(outputs, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                Loader.optimizer.step()\n",
    "\n",
    "                predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Print accuracy for epoch            \n",
    "            epoch_loss = running_loss / len(Loader.train_loader)\n",
    "\n",
    "            train_loss[epoch] = epoch_loss\n",
    "\n",
    "            epoch_accuracy = 100 * correct / total\n",
    "\n",
    "            train_accuracy[epoch] = epoch_accuracy\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Training loss: {epoch_loss:.4f}, Training accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "            #tre_loss = epoch_loss\n",
    "\n",
    "            # Evaluate the model\n",
    "            net.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            running_loss_test = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in Loader.test_loader:\n",
    "                    # Put data on GPU \n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = net(inputs)\n",
    "                    \n",
    "                    predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                    loss = Loader.loss_function(outputs, labels)\n",
    "                    \n",
    "                    running_loss_test += loss.item()\n",
    "\n",
    "                    # Count correct predictions\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                epoch_loss = running_loss_test / len(Loader.test_loader)\n",
    "                test_loss[epoch] = epoch_loss\n",
    "\n",
    "            test_accuracy_score = 100 * correct / total\n",
    "            \n",
    "            test_accuracy[epoch] = test_accuracy_score\n",
    "\n",
    "            print(f\"Test loss: {epoch_loss:.4f}, Test accuracy: {test_accuracy_score:.2f}%\")\n",
    "            print()\n",
    "\n",
    "            #scheduler.step()\n",
    "            \n",
    "            #val_loss = epoch_loss\n",
    "            #if (val_loss - tre_loss) > 0.5:\n",
    "                #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_matrix\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m, in \u001b[0;36mTrain.__init__\u001b[1;34m(self, matrix)\u001b[0m\n\u001b[0;32m     10\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     11\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 12\u001b[0m Loader \u001b[38;5;241m=\u001b[39m \u001b[43mLoad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m, in \u001b[0;36mLoad.__init__\u001b[1;34m(self, the_matrix)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,the_matrix):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthe_matrix \u001b[38;5;241m=\u001b[39m the_matrix\n\u001b[1;32m----> 5\u001b[0m     train_lsa \u001b[38;5;241m=\u001b[39m \u001b[43msentence_LSA_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tf_idf_numpy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthe_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     test_lsa \u001b[38;5;241m=\u001b[39m sentence_LSA_embedding(test_tf_idf_numpy, the_matrix)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#Tensors\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Convert inputs\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 31\u001b[0m, in \u001b[0;36msentence_LSA_embedding\u001b[1;34m(TF_IDF_matrix_numpy, V_k)\u001b[0m\n\u001b[0;32m     29\u001b[0m liste_LSA_vektorer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TF_IDF_matrix_numpy\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m---> 31\u001b[0m     lsa_vector \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTF_IDF_matrix_numpy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     liste_LSA_vektorer\u001b[38;5;241m.\u001b[39mappend(lsa_vector)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(liste_LSA_vektorer)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Train(current_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntelligentSystems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
